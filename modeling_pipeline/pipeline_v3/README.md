# Football Prediction Pipeline V3 - Complete Guide

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [System Architecture](#system-architecture)
3. [Prerequisites](#prerequisites)
4. [Installation](#installation)
5. [Data Pipeline](#data-pipeline)
6. [Feature Engineering](#feature-engineering)
7. [Model Training](#model-training)
8. [Live Prediction](#live-prediction)
9. [Monitoring & Maintenance](#monitoring--maintenance)
10. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

This is a production-ready football match prediction system that:
- Downloads historical data from SportMonks API (2015-2026)
- Engineers 159 features across multiple categories (Elo, xG, form, standings, etc.)
- Trains XGBoost model with hyperparameter tuning
- Provides live predictions for upcoming matches
- Achieves **~0.986 log loss** on future data (2025+)

### Key Features
- âœ… **159 engineered features** (optimized from 187 raw features)
- âœ… **Time-based validation** (prevents data leakage)
- âœ… **Hyperparameter tuning** (Optuna-based optimization)
- âœ… **Point-in-time standings** (no future information leakage)
- âœ… **Production-ready** (verified on 17,943 matches, 10 years of data)

### Performance Summary
- **Test Log Loss (2025+):** 0.9858
- **Test Accuracy:** 53.7%
- **Improvement over V2:** ~2.4% (V2: 1.004 log loss)
- **Top Features:** Elo difference, points difference, derived xG matchup

---

## ğŸš€ Quick Start (End-to-End)

### Prerequisites
- Python 3.11+
- SportMonks API key (Enterprise subscription)
- 8GB+ RAM, ~2GB disk space

### Step 1: Setup

```bash
cd /Users/ankurgupta/code/predictorV2/modeling_pipeline/pipeline_v3

# Install dependencies
pip install pandas numpy scikit-learn xgboost lightgbm joblib tqdm python-dotenv requests

# Configure API
echo "SPORTMONKS_API_KEY=your_key_here" > .env
```

### Step 2: Collect Data (if needed)

> **Note:** If you already have `data/csv/training_data_complete_v2.csv`, skip to Step 3.

**Option A: Quick Setup (if data exists)**
```bash
# Just verify you have the training data
ls -lh data/csv/training_data_complete_v2.csv
```

**Option B: Full Pipeline (from scratch)**

See [Complete Data Pipeline](#-complete-data-pipeline-detailed) section below for detailed steps. Quick summary:

```bash
# 1. Download historical data (30-60 min)
python3 scripts/backfill_historical_data.py \
    --leagues 8 39 140 78 135 61 62 564 \
    --start-season 2015 --end-season 2026

# 2. Convert database to CSV (1-2 min)
python3 scripts/convert_to_csv.py

# 3. Validate data quality (optional)
python3 scripts/validate_data.py

# 4. Generate training features (2-5 min)
python3 scripts/generate_training_features.py

# 5. Validate features (optional)
python3 scripts/validate_training_data_v2.py
```

**Output:** `data/csv/training_data_complete_v2.csv` (17,943 matches, 187 columns)

### Step 3: Train Model

```bash
# Train XGBoost with hyperparameter tuning (recommended)
python3 scripts/03_train_xgboost.py --tune --n-trials 30
```

**Expected Results:**
- Test Log Loss: ~0.986
- Test Accuracy: ~53.7%
- Model saved to: `models/xgboost_model.joblib`

### Step 4: Make Predictions

```python
import joblib
import pandas as pd

# Load model
model = joblib.load('models/xgboost_model.joblib')

# Load your match data
match_features = pd.read_csv('your_match_features.csv')

# Predict
probs = model.predict_proba(match_features)
print(f"Home: {probs[0][2]:.1%}, Draw: {probs[0][1]:.1%}, Away: {probs[0][0]:.1%}")
```

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 1: DATA COLLECTION                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SportMonks API â†’ SQLite Database (football.db)             â”‚
â”‚  - backfill_historical_data.py                              â”‚
â”‚  - Downloads: fixtures, lineups, statistics, standings,     â”‚
â”‚    sidelined players (2015-2026, 8 leagues)                 â”‚
â”‚  - Time: ~30-60 minutes                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 2: CSV CONVERSION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SQLite â†’ CSV Files (data/csv/)                             â”‚
â”‚  - convert_to_csv.py                                        â”‚
â”‚  - Creates: fixtures.csv, lineups.csv, statistics.csv,     â”‚
â”‚    standings.csv, sidelined.csv                             â”‚
â”‚  - Time: ~1-2 minutes                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 3: VALIDATION (Optional)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Quality Checks                                        â”‚
â”‚  - validate_data.py                                         â”‚
â”‚  - validate_features_data.py                                â”‚
â”‚  - Checks: duplicates, nulls, date ranges, FK integrity     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 4: FEATURE ENGINEERING                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CSV Files â†’ Training Dataset                               â”‚
â”‚  - generate_training_features.py                            â”‚
â”‚  - Creates 159 features: Elo, form, standings, xG, H2H      â”‚
â”‚  - Output: training_data_complete_v2.csv (17,943 matches)   â”‚
â”‚  - Time: ~2-5 minutes                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 5: MODEL TRAINING                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  XGBoost with Hyperparameter Tuning                         â”‚
â”‚  - 03_train_xgboost.py --tune --n-trials 30                 â”‚
â”‚  - Time-based split: Train<2024, Val=2024, Testâ‰¥2025        â”‚
â”‚  - Optuna optimization (30 trials)                          â”‚
â”‚  - Isotonic calibration                                     â”‚
â”‚  - Output: models/xgboost_model.joblib                      â”‚
â”‚  - Performance: 0.9858 log loss, 53.7% accuracy             â”‚
â”‚  - Time: ~2-3 minutes                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STAGE 6: PREDICTION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Load Model â†’ Generate Features â†’ Predict                   â”‚
â”‚  - For live predictions, repeat feature engineering         â”‚
â”‚    for upcoming fixtures                                    â”‚
â”‚  - Output: Home/Draw/Away probabilities                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Summary

| Stage | Script | Input | Output | Time |
|:------|:-------|:------|:-------|:-----|
| 1. Data Collection | `backfill_historical_data.py` | SportMonks API | `football.db` | 30-60 min |
| 2. CSV Export | `convert_to_csv.py` | `football.db` | CSV files | 1-2 min |
| 3. Validation | `validate_data.py` | CSV files | Validation report | 1 min |
| 4. Feature Gen | `generate_training_features.py` | CSV files | `training_data_complete_v2.csv` | 2-5 min |
| 5. Training | `03_train_xgboost.py --tune` | Training data | `xgboost_model.joblib` | 2-3 min |
| **Total** | | | | **~40-75 min** |
â”‚  Phase 2: Match Events (32 features)                        â”‚
â”‚  Phase 3: Formations (12 features)                          â”‚
â”‚  Phase 4: Injuries (16 features) â­                         â”‚
â”‚  Phase 5: Betting Odds (6 features)                         â”‚
â”‚  Phase 6: Temporal (8 features)                             â”‚
â”‚  Phase 7: Feature Selection (~200 features)                 â”‚
â”‚  + V3 Baseline (173 features)                               â”‚
â”‚  = ~296 features â†’ ~200 after selection                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MODEL TRAINING                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Phase 8: Ensemble Model                                    â”‚
â”‚  - XGBoost (60% weight)                                     â”‚
â”‚  - LightGBM (30% weight)                                    â”‚
â”‚  - Neural Network (10% weight)                              â”‚
â”‚  - Probability Calibration                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LIVE PREDICTION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Fetch upcoming fixtures â†’ Generate features â†’ Predict      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Prerequisites

### Required Software
- **Python:** 3.8+
- **SQLite:** 3.x (usually pre-installed)
- **Git:** For version control

### Required API Keys
- **SportMonks API Key:** Enterprise subscription
  - Get from: https://www.sportmonks.com/
  - Set in `.env` file

### Python Packages
```bash
# Core
pandas>=1.5.0
numpy>=1.23.0
scikit-learn>=1.2.0

# ML Models
xgboost>=1.7.0
lightgbm>=3.3.0

# Database
sqlite3 (built-in)

# API
requests>=2.28.0
python-dotenv>=0.21.0

# Utilities
joblib>=1.2.0
tqdm>=4.64.0
```

---

## ğŸš€ Installation

### Step 1: Clone Repository

```bash
cd /Users/ankurgupta/code/predictorV2/modeling_pipeline
```

### Step 2: Set Up Environment

```bash
cd pipeline_v3

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 3: Configure Environment Variables

Create `.env` file:

```bash
# .env
SPORTMONKS_API_KEY=your_api_key_here
SPORTMONKS_BASE_URL=https://api.sportmonks.com/v3/football

# Database
DATABASE_PATH=data/football.db

# Model settings
MODEL_VERSION=v3
RANDOM_SEED=42
```

### Step 4: Verify Installation

```bash
python3 -c "
from src.features.enhanced_csv_feature_engine import EnhancedCSVFeatureEngine
print('âœ… Installation successful!')
"
```

---

## ğŸ“Š Complete Data Pipeline (Detailed)

### Overview

The V3 pipeline uses a multi-stage approach:
1. **Database Setup** â†’ Create SQLite schema
2. **Data Collection** â†’ Download from SportMonks API
3. **CSV Export** â†’ Convert database to CSV files
4. **Validation** â†’ Verify data quality
5. **Feature Generation** â†’ Create training dataset

### Stage 1: Database Initialization

```bash
# Create SQLite database with proper schema
python3 scripts/create_database.sql
```

**Creates:** `data/football.db` with tables:
- `fixtures` - Match results and metadata
- `lineups` - Player lineups per match
- `events` - Goals, cards, substitutions
- `statistics` - Team statistics per match
- `sidelined` - Injured/suspended players
- `standings` - League standings (from participants.meta)

### Stage 2: Historical Data Collection

```bash
# Download historical data from SportMonks API
python3 scripts/backfill_historical_data.py \
    --leagues 8 39 140 78 135 61 62 564 \
    --start-season 2015 \
    --end-season 2026
```

**Leagues:**
- 8: Premier League
- 39: La Liga
- 140: Serie A
- 78: Bundesliga
- 135: Ligue 1
- 61: Eredivisie
- 62: Primeira Liga
- 564: Championship

**What it does:**
1. Fetches fixtures for each league/season
2. For each fixture, downloads:
   - Lineup data
   - Match events
   - Team statistics
   - Sidelined players
   - Standings (from `participants.meta`)
3. Stores in SQLite database
4. Progress saved (can resume if interrupted)

**Time:** ~30-60 minutes (API rate limits)

### Stage 3: Database to CSV Conversion

```bash
# Convert SQLite database to CSV files
python3 scripts/convert_to_csv.py
```

**Creates CSV files in `data/csv/`:**
- `fixtures.csv` (~7MB, 25,000+ rows)
- `lineups.csv` (~114MB, player-level data)
- `statistics.csv` (~6.6MB, team stats per match)
- `standings.csv` (~425KB, point-in-time standings)
- `sidelined.csv` (~1.8MB, injuries/suspensions)

**Why CSV?** Easier to inspect, version control, and process with pandas.

### Stage 4: Data Validation

```bash
# Validate CSV data quality
python3 scripts/validate_data.py
```

**Checks:**
- âœ… Required columns present
- âœ… No duplicate fixtures
- âœ… Date ranges correct
- âœ… Foreign key integrity
- âœ… Null value analysis
- âœ… Standings coverage

**Output:** `validation_report.json` with data quality metrics

**Alternative validation scripts:**
```bash
# Validate feature data specifically
python3 scripts/validate_features_data.py

# Validate training data
python3 scripts/validate_training_data_v2.py
```

### Stage 5: Feature Generation

```bash
# Generate training features from CSV files
python3 scripts/generate_training_features.py
```

**What it does:**
1. Loads all CSV files
2. For each fixture, calculates:
   - **Elo ratings** (home, away, difference, with HA)
   - **Form metrics** (last 3, 5, 10 matches)
   - **Standings** (position, points - point-in-time)
   - **xG metrics** (derived from shots, big chances)
   - **Head-to-head** (last 5 meetings)
   - **Player availability** (sidelined count)
   - **Team statistics** (possession, passing, shots)
3. Drops matches without sufficient history
4. Creates target variable (H/D/A)

**Output:** `data/csv/training_data_complete_v2.csv`
- **Rows:** 17,943 matches
- **Columns:** 187 (159 features + metadata + target)
- **Size:** ~27MB

**Time:** ~2-5 minutes

### Stage 6: Feature Validation (Optional)

```bash
# Analyze feature quality
python3 scripts/validate_features.py \
    --input data/csv/training_data_complete_v2.csv \
    --output validation_report_complete.json
```

**Checks:**
- Missing value analysis
- Feature range validation
- Distribution analysis
- Outlier detection
- Correlation analysis

**Output:** Detailed validation report with health score

---

## ğŸ” Data Quality Checks

### Quick Verification

```bash
# Check data completeness
python3 << 'EOF'
import pandas as pd

# Load training data
df = pd.read_csv('data/csv/training_data_complete_v2.csv')

print(f"Total matches: {len(df):,}")
print(f"Total features: {len(df.columns):,}")
print(f"Date range: {df['starting_at'].min()} to {df['starting_at'].max()}")
print(f"\nTarget distribution:")
print(df['result'].value_counts())
print(f"\nMissing values:")
print(df.isnull().sum().sum())
EOF
```

**Expected Output:**
```
Total matches: 17,943
Total features: 187
Date range: 2016-01-02 12:45:00 to 2025-12-30 20:15:00

Target distribution:
H    7933
A    5529
D    4481

Missing values: (varies by feature)
```

### Standings Coverage Check

```bash
# Verify point-in-time standings
python3 scripts/compare_standings_coverage.py
```

**Verifies:**
- Standings are point-in-time (no future leakage)
- Coverage % per league/season
- Identifies missing standings data
    --seasons 2016-2024 \
    --batch-size 100
```

**Parameters:**
- `--leagues`: League IDs (8 = Premier League, 384 = La Liga, etc.)
- `--seasons`: Season range (YYYY-YYYY)
- `--batch-size`: API requests per batch (default: 100)

**What it downloads:**
1. **Fixtures:** Match results, dates, teams
2. **Lineups:** Player lineups with detail_* stats (ratings, touches, passes)
3. **Events:** Goals, cards, substitutions with timestamps
4. **Formations:** Team formations (4-3-3, 4-4-2, etc.)
5. **Odds:** Pre-match betting odds (1x2)
6. **Sidelined:** Injuries and suspensions
7. **Standings:** League positions and points (from participants.meta) â­ **NEW**

**Progress tracking:**
```
Downloading fixtures: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3800/3800 [15:23<00:00]
Downloading lineups: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3800/3800 [42:15<00:00]
Downloading events: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3800/3800 [28:45<00:00]
...
âœ… Download complete: 3800 fixtures
```

**Estimated time:**
- Premier League (1 season): ~2 hours
- Premier League (8 seasons): ~12-16 hours
- Multiple leagues: Scale accordingly

### Step 3: Export to CSV

```bash
# Export JSON to CSV files (V2 - Robust with validation)
python3 scripts/convert_json_to_csv_v2.py
```

**Features:**
- âœ… Comprehensive error handling and logging
- âœ… Data validation (numeric parsing, range checks)
- âœ… Automatic standings extraction from `participants.meta`
- âœ… Detailed conversion statistics
- âœ… Log file: `json_to_csv_conversion.log`


**Output:** `data/csv/`
- `fixtures.csv` (3800 rows)
- `lineups.csv` (83,600 rows - 22 players Ã— 3800 matches)
- `events.csv` (152,000 rows - ~40 events Ã— 3800 matches)
- `formations.csv` (7,600 rows - 2 teams Ã— 3800 matches)
- `odds.csv` (11,400 rows - 3 outcomes Ã— 3800 matches)
- `sidelined.csv` (varies)
- `standings.csv` (7,600 rows - 2 teams Ã— 3800 matches) â­ **NEW**

> **Note:** The V2 converter automatically extracts standings from the API's `participants.meta` field. This provides **official league standings** (position, points, wins, draws, losses, GF, GA, GD) which are more accurate than calculating from match results. No separate conversion step needed!

### Step 4: Verify Data Quality

```bash
# Comprehensive data validation with random sampling
python3 scripts/validate_data.py
```

**What it validates:**
- âœ… **Fixtures:** Team IDs, scores, results match JSON
- âœ… **Standings:** Position, points, W/D/L, GF/GA/GD match `participants.meta` â­
- âœ… **Lineups:** Player count, IDs, starter status match JSON
- âœ… **Statistics:** Team count, possession totals are sane
- âœ… **Data Quality:** No missing columns, acceptable null rates, no duplicates

**Method:** Random sampling (20 fixtures) + cross-validation against original JSON files

---

## ğŸ”§ Feature Engineering

### Overview

The feature engineering pipeline generates 296 features across 8 phases, then optimizes to ~200 features.

### Architecture

```python
EnhancedCSVFeatureEngine
â”œâ”€â”€ ComprehensiveCSVFeatureEngine (V3 Baseline - 173 features)
â”œâ”€â”€ PlayerStatisticsExtractor (Phase 1 - 49 features)
â”œâ”€â”€ EventFeatureExtractor (Phase 2 - 32 features)
â”œâ”€â”€ FormationFeatureExtractor (Phase 3 - 12 features)
â”œâ”€â”€ InjuryFeatureExtractor (Phase 4 - 16 features) â­
â”œâ”€â”€ OddsFeatureExtractor (Phase 5 - 6 features)
â””â”€â”€ TemporalFeatureExtractor (Phase 6 - 8 features)
```

### Feature Categories

#### Phase 0: V3 Baseline (173 features)
- Elo ratings
- League position & points (from API standings) â­
- Form metrics (L3, L5, L10)
- Head-to-head statistics
- xG (expected goals)
- Shots, possession, passing
- Defense metrics
- Momentum & trajectory

**Note:** League position features now use **official standings from SportMonks API** (`participants.meta`) instead of calculating from match results. This provides:
- âœ… More accurate positions (handles all tiebreaker rules)
- âœ… Additional features: wins, draws, losses, GF, GA, GD
- âœ… 100x faster lookup vs calculation

#### Phase 1: Player Statistics (49 features)
```python
# Core stats (per team, L5 window):
- rating_avg_5, rating_max_5, rating_min_5, rating_std_5
- touches_avg_5, touches_max_5, touches_min_5
- passes_total_avg_5, pass_accuracy_avg_5
- duels_won_avg_5, duels_total_avg_5
- shots_avg_5, shots_on_target_avg_5
- key_passes_avg_5

# Derived metrics:
- top_3_rating_avg_5 (best 3 players)
- team_pass_completion_5
- team_duel_success_5

# Relative features:
- player_rating_diff_5 (home - away)
```

**Data source:** `lineups.csv` (detail_118, detail_80, etc.)

#### Phase 2: Match Events (32 features)
```python
# Goal timing (per team):
- goals_first_15min (early pressure)
- goals_15_45min (first half)
- goals_second_half
- goals_last_15min (late goals)
- late_goals_conceded (defensive lapses)
- early_pressure (binary)

# Discipline:
- yellow_cards, red_cards
- cards_per_match
- early_cards (0-30 min)
- discipline_score
- has_discipline_issue (binary)

# Substitutions:
- subs_before_60min (tactical)
- subs_after_75min (time wasting)
- avg_subs_per_match
- tactical_flexibility (binary)
```

**Data source:** `events.csv` (type_id: 14=goal, 15=yellow, 16=red, 18=sub)

#### Phase 3: Formations (12 features)
```python
# Per team:
- formation_consistency (% same formation)
- formation_changes (number of changes)
- primary_formation_encoded (most used)
- defensive_formation_rate (% 5-4-1, 4-5-1)
- attacking_formation_rate (% 4-3-3, 3-4-3)
- balanced_formation_rate (% 4-4-2)
```

**Data source:** `formations.csv`

#### Phase 4: Injuries (16 features) â­ CRITICAL
```python
# Per team:
- injuries_count (total unavailable)
- injuries_long_term (out > 4 weeks)
- injury_crisis (>= 3 injuries)
- key_players_missing (star players out)
- lineup_strength_ratio (available / full)
- attack_strength_impact (missing forwards)
- defense_strength_impact (missing defenders)
- midfield_strength_impact (missing midfielders)
```

**Key player identification:**
```python
# From historical lineups:
key_players = players with:
    - avg_rating > 7.5 (top performers)
    - games_played >= 10 (regulars)
    - avg_minutes > 70 (starters)
```

**Data source:** `sidelined.csv` + `lineups.csv`

#### Phase 5: Betting Odds (6 features)
```python
# Market probabilities:
- bookmaker_home_win_prob
- bookmaker_draw_prob
- bookmaker_away_win_prob
- bookmaker_favorite (1=home, 0=draw, -1=away)
- bookmaker_confidence (margin)
- market_efficiency (bookmaker margin)
```

**Temporal cutoff:** 24+ hours before match (prevents data leakage)

**Data source:** `odds.csv`

#### Phase 6: Temporal (8 features)
```python
# Per team:
- days_since_last_match (rest days)
- matches_in_7_days (fixture density)
- fixture_congestion_score (fatigue)
- season_phase (0=start, 1=end)
```

**Data source:** `fixtures.csv`

### Generate Training Data

```bash
# Generate features for all fixtures
python3 scripts/regenerate_training_data.py \
    --start-date 2016-08-01 \
    --end-date 2024-05-31 \
    --output data/processed/training_data.csv
```

**Process:**
1. Load CSV data
2. For each fixture:
   - Extract fixture details
   - Generate all 296 features
   - Validate for data leakage
   - Save to output
3. Progress tracking

**Output:**
```csv
fixture_id,date,home_team_id,away_team_id,result,home_elo,away_elo,...,home_injuries_count,away_injuries_count
5590,2016-03-12,52,30,1,1542.3,1489.7,...,2,0
...
```

**Estimated time:**
- 3800 fixtures: ~30-45 minutes
- Progress: ~2-3 fixtures/second

### Validate Feature Quality â­ **NEW**

```bash
# Comprehensive feature validation
python3 scripts/validate_features_data.py
```

**What it validates:**
- âœ… **Feature Completeness:** All expected features present (200-300 features)
- âœ… **Data Quality:** No nulls, infinities, or extreme outliers
- âœ… **Target Distribution:** Balanced classes (H/D/A)
- âœ… **Data Leakage:** No future data in features
- âœ… **Feature Correlations:** No redundant features (>0.95 correlation)

**Checks performed:**
1. Feature count in expected range (200-300)
2. Null values <10% per feature
3. No infinite values
4. No constant features (zero variance)
5. Extreme outliers <1% per feature
6. Target variable has valid values (H/D/A)
7. No perfect correlations with target (>0.99)
8. Highly correlated feature pairs identified

**Output:**
```
ğŸ‰ ALL VALIDATIONS PASSED!
Training data is ready for model training
```

---


## ğŸ¤– Model Training

### Recommended: XGBoost with Hyperparameter Tuning

Based on extensive testing, **standalone XGBoost with hyperparameter tuning** provides the best performance (0.9858 log loss on 2025+ data).

```bash
# Train with hyperparameter tuning (30 trials, ~2-3 minutes)
python3 scripts/03_train_xgboost.py --tune --n-trials 30
```

**What it does:**
1. Loads `data/csv/training_data_complete_v2.csv`
2. **Time-based split:** Train < 2024, Val = 2024, Test â‰¥ 2025
3. Drops constant columns and data leakage features
4. Runs Optuna hyperparameter search (30 trials)
5. Trains final model with best parameters
6. Calibrates probabilities using isotonic regression
7. Evaluates on test set (2025+)

**Best Hyperparameters Found:**
```python
{
    'max_depth': 8,
    'learning_rate': 0.03,
    'subsample': 0.9,
    'colsample_bytree': 0.6,
    'min_child_weight': 3,
    'gamma': 0.5,
    'reg_alpha': 1.0,
    'reg_lambda': 2.0
}
```

**Expected Output:**
```
FINAL TEST SET EVALUATION (2025)
Test Log Loss:       0.9858
Test Accuracy:       53.7%

Top 15 Features:
  elo_diff                           17.5
  elo_diff_with_home_advantage       16.3
  points_diff                         5.9
  derived_xgd_matchup                 4.4
  ...

Model saved to: models/xgboost_model.joblib
```

### Alternative: Train Without Tuning (Faster)

```bash
# Use default hyperparameters (~30 seconds)
python3 scripts/03_train_xgboost.py
```

**Performance:** ~1.036 log loss (slightly worse than tuned)

### Not Recommended: Ensemble Model

Our testing shows the ensemble (XGBoost + LightGBM + Neural Network) performs **worse** than standalone XGBoost:
- **Ensemble:** 1.0103 log loss
- **XGBoost (tuned):** 0.9858 log loss âœ… **Better**

The Neural Network (1.0438 log loss) drags down the ensemble performance.

---

## ğŸ“ˆ Performance Benchmarks

### Test Set Performance (2025+ Data)

| Model | Log Loss | Accuracy | Notes |
|:------|:---------|:---------|:------|
| **XGBoost (tuned)** | **0.9858** | 53.7% | âœ… **Recommended** |
| XGBoost (default) | 1.0360 | 45.9% | Faster but worse |
| Ensemble (XGB+LGB+NN) | 1.0103 | 53.0% | More complex, worse performance |
| V2 (fixed, no leakage) | 1.0043 | 51.1% | Previous version |

### Feature Importance (Top 10)

| Feature | Importance | Category |
|:--------|:-----------|:---------|
| `elo_diff` | 17.5 | Elo Rating |
| `elo_diff_with_home_advantage` | 16.3 | Elo Rating |
| `points_diff` | 5.9 | Standings |
| `derived_xgd_matchup` | 4.4 | xG Metrics |
| `away_elo` | 4.1 | Elo Rating |
| `home_elo_vs_league_avg` | 3.9 | Elo Rating |
| `away_elo_vs_league_avg` | 3.7 | Elo Rating |
| `position_diff` | 3.6 | Standings |
| `home_elo` | 3.5 | Elo Rating |
| `away_pass_accuracy_5` | 3.4 | Team Stats |

**Key Insight:** Elo ratings dominate (50%+ of importance), followed by standings and xG metrics. No data leakage detected.
```

**Calibration:**
```python
# Isotonic calibration on validation set
calibrated = CalibratedClassifierCV(
    ensemble, method='isotonic', cv='prefit'
)
```

**Output:**


**Files created:**
- `models/ensemble_model.pkl` - Production model

**Estimated training time:**
- Feature selection: ~15-30 minutes
- Ensemble training: ~20-40 minutes
- Total: ~45-70 minutes

---

## ğŸ”® Live Prediction

### Setup

```bash
# Ensure models are trained
ls models/
# Should see:
# - ensemble_model.pkl
# - feature_selector.pkl
```

### Predict Upcoming Matches

```bash
# Predict matches for tomorrow
python3 scripts/predict_live.py --date tomorrow
```

**Process:**
1. Fetch upcoming fixtures from API
2. For each fixture:
   - Generate all 296 features
   - Apply feature selection
   - Predict with ensemble
3. Output predictions

**Output:**
```
LIVE MATCH PREDICTIONS FOR 2026-01-29
======================================

Match: Liverpool vs Manchester United
  Home Win: 58.3%
  Draw:     23.4%
  Away Win: 18.3%
  Recommendation: BET HOME (confidence: HIGH)

Match: Arsenal vs Chelsea
  Home Win: 45.2%
  Draw:     28.9%
  Away Win: 25.9%
  Recommendation: NO BET (confidence: LOW)

...

Completed 10 predictions
```

### API Integration

```python
# Use in your application
from src.models.ensemble_model import load_ensemble
from src.features.enhanced_csv_feature_engine import EnhancedCSVFeatureEngine

# Load model
ensemble = load_ensemble('models/ensemble_model.pkl')
engine = EnhancedCSVFeatureEngine('data/csv')

# Generate features for upcoming match
features = engine.generate_features_for_fixture(
    fixture_id=upcoming_fixture_id,
    as_of_date='2026-01-29'
)

# Predict
prediction = ensemble.predict_proba([list(features.values())])

print(f"Home Win: {prediction[0][2]:.1%}")
print(f"Draw: {prediction[0][1]:.1%}")
print(f"Away Win: {prediction[0][0]:.1%}")
```

---

## ğŸ“ˆ Monitoring & Maintenance

### Performance Tracking

```bash
# Evaluate on recent matches
python3 scripts/evaluate_recent.py --days 30
```

**Metrics:**
- Log loss
- Accuracy
- Calibration
- ROI (if betting)

### Model Retraining

**When to retrain:**
- Every 3-6 months (seasonal changes)
- When performance degrades (log loss > 0.95)
- After major rule changes
- New data available

**Process:**
```bash
# 1. Download new data
python3 scripts/backfill_historical_data.py --seasons 2024-2025

# 2. Regenerate training data
python3 scripts/regenerate_training_data.py

# 3. Retrain models
python3 scripts/train_with_feature_selection.py
python3 scripts/train_ensemble_model.py

# 4. Validate
python3 scripts/validate_model.py
```

### Data Updates

```bash
# Daily: Update recent fixtures
python3 scripts/update_recent_data.py --days 7

# Weekly: Full sync
python3 scripts/sync_all_data.py
```

---

## ğŸ› Troubleshooting

### Common Issues

#### 1. API Rate Limiting
**Error:** `429 Too Many Requests`

**Solution:**
```python
# In backfill_historical_data.py
RATE_LIMIT_DELAY = 2  # Increase delay between requests
```

#### 2. Missing Features
**Error:** `KeyError: 'home_rating_avg_5'`

**Solution:**
```bash
# Check if lineups data exists
python3 -c "
import pandas as pd
lineups = pd.read_csv('data/csv/lineups.csv')
print(f'Lineups: {len(lineups)} rows')
print(f'Columns: {lineups.columns.tolist()}')
"
```

#### 3. Memory Issues
**Error:** `MemoryError`

**Solution:**
```python
# Process in batches
python3 scripts/regenerate_training_data.py --batch-size 500
```

#### 4. Model Not Loading
**Error:** `FileNotFoundError: models/ensemble_model.pkl`

**Solution:**
```bash
# Verify models exist
ls -lh models/

# Retrain if missing
python3 scripts/train_ensemble_model.py
```

### Logs

```bash
# Check logs
tail -f logs/pipeline.log

# Debug mode
python3 scripts/regenerate_training_data.py --debug
```

---

## ğŸ“š Additional Resources

### Documentation
- [SportMonks API Docs](https://docs.sportmonks.com/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Scikit-learn Guide](https://scikit-learn.org/stable/)

### Support
- GitHub Issues: [Create issue](https://github.com/your-repo/issues)
- Email: support@example.com

---

## ğŸ‰ Quick Start Summary

```bash
# 1. Setup
cd pipeline_v3
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Configure
cp .env.example .env
# Edit .env with your API key

# 3. Download data (12-16 hours)
python3 scripts/backfill_historical_data.py --leagues 8 --seasons 2016-2024

# 4. Export to CSV (5 minutes)
python3 scripts/export_to_csv.py

# 5. Generate features (30-45 minutes)
python3 scripts/regenerate_training_data.py

# 6. Train models (45-70 minutes)
python3 scripts/train_with_feature_selection.py
python3 scripts/train_ensemble_model.py

# 7. Predict!
python3 scripts/predict_live.py --date tomorrow
```

**Total time:** ~14-18 hours (mostly data download)

---

## âœ… Success Criteria

- âœ… Log loss < 0.923 (target: 0.915-0.923)
- âœ… Exceeds parent pipeline (0.9478)
- âœ… All 296 features generated successfully
- âœ… Feature selection reduces to ~200 features
- âœ… Ensemble improves over individual models
- âœ… Live predictions work without errors
- âœ… No data leakage detected

---

**Version:** 3.0  
**Last Updated:** 2026-01-28  
**Status:** Production Ready âœ…
